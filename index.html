<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Datastore Client APIs</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/dracula.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/vs2015.css">
		<style>
			.na_ish {
				opacity: 0.5;
			}
			.shrink_strike_fade {
				font-size: 90%;
				text-decoration: line-through;
				opacity: 0.5;
			}
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-state="title-1">
					<h1>Datastore<br>
						Client APIs</h1>
					<p>The app dev's datastore <br>
						🛒 shopping guide</p>
				</section>

				<section>
							<p>Akira Kurogane</p>
							<p style="font-size: 75%;">Distributed Data Specialist @ Marionete</p>
							<ul>
								<li>MongoDB expert / Software industry</li>
								<li>Distributed Search Engine developer / eCommerce</li>
								<li>Application development / Finance industry</li>
							</ul>
				</section>

				<section><!-- Overview -->
					<h3>Overview</h3>
					<ul>
						<li>What are Datastore Client APIs?</li>
						<li>Productivity & Tech Debt</li>
						<li>APIs grouped by main styles</li>
						<li>Pros and cons of features X, Y & Z ...</li>
						<li>Connection protocols</li>
						<li>Security</li>
					</ul>
				</section>

				<section><!-- What is a Datastore API? -->
					<h3>What is a Datastore API?</h3>
					<p class="fragment">Libraries you import/include in your program to interface with the external datastore</p>
					<ul class="fragment">
						<li>ODBC 'driver'</li>
						<li>NoSQL database or KV API</li>
						<li>HTTP REST API</li>
						<li>Embedded storage engine API</li>
						<li>Filesystem API</li>
						<li>etc.</li>
					</ul>
				</section>

				<section>
					<p>The API will include functions to:</p>
					<ul>
						<li>Open network connection to datastore</li>
						<li>Authenticate</li>
						<li>Query</li>
						<li>Insert/Update/Delete</li>
						<li>Iterate records</li>
					</ul>
					<p>... and more, varying according to the datastore.</p>
				</section>

				<section>
					<p>Old days: "Database driver"</p>
					<p>(Not true drivers though)</p>
					<p>Recently also called the following:</p>
					<ul>
						<li>Client library</li>
						<li>API</li>
						<li>&lt;Product name&gt; SDK</li>
					<aside class="notes">
						Not an OS kernel module that that manages a hardware device such as disk drives, network interface card, graphics, USB hub, etc.
					</aside>
				</section>

				<section data-auto-animate><!-- Gruen Effect -->
					<h2>The Gruen Effect</h2>
					<h4 data-auto-animate-id="gest">...</h4>
					<p data-auto-animate-id="geq">Consumers enter a shopping mall or store and, surrounded by an intentionally confusing layout, lose track of their original intentions, making them more susceptible to impulse buys</p>
				</section>

				<section data-auto-animate><!-- Gruen Effect -->
					<h2>The Gruen Effect</h2>
					<h4 data-auto-animate-id="gest">Software Architecture Edition</h4>
					<p data-auto-animate-id="geq">Developers search the web and, surrounded by a confusing array of options, lose track of their original requirements, making them susceptible to accepting whatever solution they browsed last.</p>
				</section>

				<section>
					<h1>Productivity & Tech Debt</h1>
				</section>
				<section>
					<h2>Developer (un)productivity</h2>
				</section>
				<section>
					<p>No matter which app, the <i>purpose and style</i> of the application's code is not the same as the interface of datastore's API.</p>
					<p>This <i>friction</i> breaks the developer's concentration and slows the release time of every update that changes the datastore interaction.​</p>
				</section>
				<section>
					<p>The disruption level varies according to which API is being used.</p>
					<p>Some APIs are simple, and close to an application programming language's normal conventions.</p>
					<p>Others are their own rich, separate domain to the application.</p>
				</section>
				<section>
					<p>Pick the datastore with the best client API for your staff and application requirements<br>
						=<br>
						Development and release cycle is faster each time</p>
				</section>
				<section>
					<h2>Technical debt</h2>
					<p>If you use it, you require other developers to have or pick up the same knowledge about it before they can contribute.</p>
				</section>
				<section>
					<p>Picking the simplest client API that meets requirements<br>
						=<br>
						Lower application tech debt in future</p>
				</section>

				<section><!-- Styles of Datastore APIs -->
					<h1 style="">Grouping APIs by<br/>'Look and feel'</h1>
				</section>

				<section>
					<h2>Main styles of Datastore APIs</h2>
					<ul style="display: inline-block">
						<li>SQL, Relational</li>
						<li>Document databases</li>
						<li>Object stores</li>
						<li>Spartan Key-Value</li>
						<li>Timeseries</li>
					</ul>
					<ul style="display: inline-flexbox">
						<li>Search Engines</li>
						<li>Graph</li>
						<li>Native container wrappers</li>
						<li>Embedded storage engine</li>
						<li>Filesystem</li>
					</ul>
				</section>

				<section><!-- SQL -->
					<h1>SQL</h1>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1|2-4|6-9|11-13|1-13">
						import xyz_odbc_lib
						conn = xyz_odbc_lib.connect(dbname=business_app_foo, 
							user=appservice_acct_pqr, ...)
						cur = conn.cursor()

						cur.execute(
							"INSERT INTO test (catId, data) VALUES (%s, %s)", 
							(14, "abcdef"))
						conn.commit()

						cur.execute("SELECT fooId, catId, data FROM test")
						print(cur.fetchone())
						# (1, 14, "abcdef")
					</code></pre>
					<aside class="notes">
						1.  GRANT CREATE TABLE … is usually only given to an account the DBA manually logs in with. The table object needs to be created first, and the expectations of the app must be in sync with what the table definition was made to be by the DBA.
						2. SQL statements are strings that will be parsed server-side. Strings that are a computer language, inside your application code in another programming language.
						3. So the app must format all vars into the string before sending, which is awkward code.
						4. Transactions are used more in relational databases because normalized form often puts one business object in several tables. Multi-table transactions drive the latency of one op for the client, and place concurrency-guaranteeing burdens in the db server. Transactions with writes can block other clients too.
						5. Again, SQL string in the middle of different language.
						6. To use the returned results is typically more verbose than this example. Column names and types are metadata of the cursor result. Records don't have the column names as keys in them. It can be the other way but the original style of the cursor API is like a table - fixed cols and types, rows are all tuples of those same types they don't need the metadata each.
					</aside>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="2-6|7|9-15|1-15">
						-- CREATE TABLE foo ...
						CREATE TABLE IF NOT EXISTS categories (
							INT catId NOT NULL,
							VARCHAR(48) catName NOT NULL,
							PRIMARY KEY (catId)
						);
						-- INSERT INTO categories VALUES (..., '...'), (..., '...''), ...;

						CREATE TABLE IF NOT EXISTS foo (
							fooId INT NOT NULL AUTO_INCREMENT,
							catId INT NOT NULL,
							data VARCHAR(2056),
							PRIMARY KEY (fooId),
							FOREIGN KEY (catId) REFERENCES categories(catId)
						);
					</code></pre>
				</section>
				<section>
					<pre><code data-trim>
						import xyz_odbc_lib
						conn = xyz_odbc_lib.connect(dbname=business_app_foo, 
							user=appservice_acct_pqr, ...)
						cur = conn.cursor()

						cur.execute(
							"INSERT INTO test (catId, data) VALUES (%s, %s)", 
							(14, "abcdef"))
						conn.commit()

						cur.execute("SELECT fooId, catId, data FROM test")
						print(cur.fetchone())
						// (1, 14, "abcdef")
					</code></pre>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1|3-10|12-14|1-16">
						// ... start wrapping ...

						def insertFooRec(cur, catId, data):
							cur.execute(
								"INSERT INTO foo (catId, data) VALUES (%s, %s)", 
								(catId, data))
							newFooId	= cursor.fetchone()[0]
							// or "SELECT @@IDENTITY AS fooId"
							conn.commit()
							return newFooId

						def getFooRec(cur, fooId):
							cur.execute("SELECT fooId, catId, data FROM foo")
							return cur.fetchone()
						
						// ...
					</code></pre>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1-11|1-4|6|8-10|1-11">
						import xyz_odbc_lib
						conn = xyz_odbc_lib.connect(dbname=business_app_foo, 
							user=appservice_acct_pqr, ...)
						cur = conn.cursor()

						newId = insertFooRec(cur, 14, "abcdef")

						rec = getFooRec(cur, newId)

						print(rec)
						// (1, 14, "abcdef")
					</code></pre>
				</section>
				<section>
					<h3>Other common tasks to wrap:</h3>
					<ul>
						<li>BEGIN / COMMIT / ABORT transaction</li>
						<li>Parent and child row inserts</li>
					</ul>
				</section>
				<section>
					<h3>Move logic to the DBMS?</h3>
					<h4>Stored procedures</h4>
					<p>Written in a limited, non-portable DSL.<br/>Slow despite being in database.</p>
					<h4>Aggregations</h4>
					<p>Typically easy to embed, but ...</p>
				</section>
				<section><!-- Postgresql SELECT statement reference -->
					<pre><code data-trim>
						[ WITH [ RECURSIVE ] with_query [, ...] ]
						SELECT [ ALL | DISTINCT [ ON ( expression [, ...] ) ] ]
							[ * | expression [ [ AS ] output_name ] [, ...] ]
							[ FROM from_item [, ...] ]
							[ WHERE condition ]
							[ GROUP BY [ ALL | DISTINCT ] grouping_element [, ...] ]
							[ HAVING condition ]
							[ WINDOW window_name AS ( window_definition ) [, ...] ]
							[ { UNION | INTERSECT | EXCEPT } [ ALL | DISTINCT ] select ]
							[ ORDER BY expression [ ASC | DESC | USING operator ] [ NULLS { FIRST | LAST } ] [, ...] ]
							[ LIMIT { count | ALL } ]
							[ OFFSET start [ ROW | ROWS ] ]
							[ FETCH { FIRST | NEXT } [ count ] { ROW | ROWS } { ONLY | WITH TIES } ]
							[ FOR { UPDATE | NO KEY UPDATE | SHARE | KEY SHARE } [ OF table_name [, ...] ] [ NOWAIT | SKIP LOCKED ] [...] ]
						
						where from_item can be one of:
						
							[ ONLY ] table_name [ * ] [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
										[ TABLESAMPLE sampling_method ( argument [, ...] ) [ REPEATABLE ( seed ) ] ]
							[ LATERAL ] ( select ) [ AS ] alias [ ( column_alias [, ...] ) ]
							with_query_name [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
							[ LATERAL ] function_name ( [ argument [, ...] ] )
										[ WITH ORDINALITY ] [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
							[ LATERAL ] function_name ( [ argument [, ...] ] ) [ AS ] alias ( column_definition [, ...] )
							[ LATERAL ] function_name ( [ argument [, ...] ] ) AS ( column_definition [, ...] )
							[ LATERAL ] ROWS FROM( function_name ( [ argument [, ...] ] ) [ AS ( column_definition [, ...] ) ] [, ...] )
										[ WITH ORDINALITY ] [ [ AS ] alias [ ( column_alias [, ...] ) ] ]
							from_item join_type from_item { ON join_condition | USING ( join_column [, ...] ) [ AS join_using_alias ] }
							from_item NATURAL join_type from_item
							from_item CROSS JOIN from_item
						
						and grouping_element can be one of:
						
							( )
							expression
							( expression [, ...] )
							ROLLUP ( { expression | ( expression [, ...] ) } [, ...] )
							CUBE ( { expression | ( expression [, ...] ) } [, ...] )
							GROUPING SETS ( grouping_element [, ...] )
						
						and with_query is:
						
							with_query_name [ ( column_name [, ...] ) ] AS [ [ NOT ] MATERIALIZED ] ( select | values | insert | update | delete )
								[ SEARCH { BREADTH | DEPTH } FIRST BY column_name [, ...] SET search_seq_col_name ]
								[ CYCLE column_name [, ...] SET cycle_mark_col_name [ TO cycle_mark_value DEFAULT cycle_mark_default ] USING cycle_path_col_name ]
						
						TABLE [ ONLY ] table_name [ * ]
					</code></pre>
				</section>
				<section>
					<h3>SQL summary</h3>
					<ul>
						<li>SQL syntax a separate language</li>
						<li>Relational table concept simple, suits many</li>
						<li>SQL 'standard' only for basic levels</li>
						<li>Higher level syntax untuitive, arbitrary</li>
						<li>Large, complex aggregations supported</li>
					</ul>
					<aside class="notes">
						<ul>
							<li>Programmer is obliged to learn SQL syntax and manage it, a separate language, as strings inside the application's actual programming language.</li>
							<li>SQL is widely used, has a standard that no-one keeps perfectly but most are close.</li>
							<li>Relational table concept is simple and appealing; suits a lot of cases.</li>
							<li>SQL is first intuitive and easy; quickly becomes non-intuitive beyond easy level.</li>
							<li>Complex, large aggregations can be done server-side.</li>
						</ul>
					</aside>
				</section>
				<section>
					<h3>SQL summary</h3>
					<ul>
						<li>DB schema change deployments slow</li>
						<li>App programmer sees the DB as rigid</li>
						<li>Slow multi-table transactions the norm</li>
						<!-- li>Generic programming around SQL difficult</li -->
					</ul>
					<aside class="notes">
						<ul>
							<li>DB schema changes in the DBMS will require synchronous app version updates.</li>
							<li>App programmer sees the DB as a rigid thing.</li>
							<li>(Not shown) writing a generic function that dynamically accepts any table's record definitions is verbose and time-consuming to write, and hard to get right.</li>
							<li>Relational table normalized forms require multi-table operations within transactions.</li>
						</ul>
					</aside>
				</section>

				<section><!-- Doc DBs-->
					<h1>Document Database</h1>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1-3|5-7|9-10|12-14|1-14">
						import xyz_db_lib
						db = xyz_db_lib.connect(dbname=business_app_foo,
																		user=appservice_acct_pqr)
						
						cr_array = [ "Xxx", "Yyy", "Zzz" ]
						x = { cat_id: 14, desc: "...", 
									child_recs: cr_array, created: date.now() }
						
						db.foo_collection.insert(x)
						//x._id property added automatically

						print(db.foo_collection.findOne(x._id)
						> { _id: "4251e8ff-ec...e68", cat_id: 14, desc: "...",
						    child_recs: [ "Xxx", "Yyy", "Zzz" ], created: ... }
					</code></pre>
					<aside class="notes">
						<ul>
							<li>1. Native variable types of the programming language are used as-is (so long as they are a datatype in the db). No translation to another type needed. It's just your object, no transformation to a record type is required. When you fetch it back it will be the same without extra deserialization code needing to be written. However, if your code has a datatype that the db doesn't support that is another matter.</li>
							<li>2. Typically the db will auto-create table/collections as soon as a write command is executed on them</li>
							<li>3. Typically any schema object can be inserted. This means the app devs can change the schema without DBA requests.</li>
							<li>4. Queries are with objects (this case a simple 'fieldX = valueY' rule by way of map with a single key->value pair) of the programming language</li>
							<li>5. The results come back as a native object. Not a record tuple type which is a little bit more verbose</li>
						</ul>
					</aside>
				</section>
				<section>
					<h3>No shared API standards</h3>
					<p>Every document database has<br/>an individual API syntax</p>
					<Cassandra: class="footnote">(A few API-compatible 'clones' or proxy services exist. MongoDB: Amazon DocumentDB, Azure CosmosDB. Cassandra: ScyllaDB)</p>
				</section>
				<section>
					<h3>Transactions not encouraged</h3>
					<p>Most don't support at all. But if supported,<br/>syntax is idiomatic to app's language.</p>
					<pre><code data-trim>
						# one example ...
						response = client.transact_write_items(
							TransactItems=[
									{ ... },
									{ ... }
							]
						)
					</code></pre>
				</section>
				<section>
					<p>No stored procedures</p>
					<p>Aggregations supported roughly equal to RDBMS,<br/>but the syntax is different</p>
				</section>
				<section>
					<ul>
						<li>APIs idiomatic with app language</li>
						<li>Individual APIs for each doc db</li>
						<li>Free field schema</li>
						<li>Automatic table creation, incl. default PK</li>
					</ul>
				</section>
				<section>
					<div>(cont.)</div>
					<ul>
						<li>Nested arrays, maps in records normal</li>
						<li>Schema constraints supported, but non-typical</li>
						<li>Table joins supported, but non-typical</li>
					</ul>
					<aside class="notes">
						<ul>
							<li>Some management of table and indexes still needed, but it is not as blocking in the release effort</li>
						</ul>
					</aside>
				</section>

				<section><!-- Object Stores -->
					<h1>Object Stores</h1>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1-4|6|8|10-11|13-15|1-15">
						import xyz_cloudapi_lib
						client = xyz_cloudapi_lib.connect()
						client.auth(client_key: xxxx, secret: …...)
						obj_api = client.api_select('object_store')

						foo_bucket = obj_api.select_bucket('foo')

						x = # any sort of application object 

						x_key = 'someprefix/' + str(x.id)
						foo_bucket.upload_object(x_key, x.serialize())

						y_key = 'someprefix/' + y_id
						y = deserialize(foo_bucket.get_object('y_key)))
					</code></pre>
					<aside class="notes">
						1. You're making a HTTP connection and every operation is a HTTP request. So firstly each request is slower than a db driver like ODBC or mongo wire protocol. Secondly Error handling will be HTTP error handling, or getting a well-formed but proprietary error response to parse and deal with.​
						2. Stricter authentication at the base level; no simple username+password option
						3. Might be one of many services in a cloud provider
						4. Buckets. Equivalent organizational entity to collection/tables.
						5. You need to choose a key/path for each object. Need to share that same logic amongst all app code.
						6. You have to serialize and deserialize the objects. Some common formats such as JSON will have simple ways to do it though
						7. No object format (schema control) enforced by the API
						8. There is no query support such 'find objects with field_A = value_X'. You can only retrieve or update by the primary key.
						(Amazon's S3 Select, Athena or Redshift are layers on top of S3 that run close to S3 object storage servers, they are not object stores themself.)
					</aside>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1-11|6-8|1-11">
						# file: cloudapi_common
						import xyz_cloudapi_lib

						xyz_cloudapi_client _client

						def init_cloud_auth:
							_client = xyz_cloudapi_lib.connect() # HTTP 
							_client.auth(_client: xxxx, secret: …...) # HTTP session

						def obj_store:
							return _client.object_store
					</code></pre>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="2|4-7|9-10|12-18|1-18">
						# file: obj_store_deser
						import json # or avro, bson, csv, etc., etc.

						class FooSerializationError(Exception):
							...
						class FooDeserializationError(Exception):
							...
						
						def validateFooSchema(val):
							...
						
						def serializeFoo(val):
							if !validateFooSchema(val):
								throw FooSerializationError
							return json.dumps(val)
						
						def deserializeFoo(str_val):
							...
					</code></pre>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1-2|1-11">
						import cloudapi_common
						import obj_store_deser
						
						foo_bucket = obj_store.select_bucket('foo')

						x = new Foo(...) 
						x_key = 'someprefix/' + str(x.id)
						foo_bucket.upload_object(x_key, serializeFoo(x))

						y_key = 'someprefix/' + y_id
						y = deserializeFoo(foo_bucket.get_object('y_key))
					</code></pre>
				</section>
				<section>
					<ul>
						<li>General feeling is 'File put/get over HTTP'</li>
						<li>But atomic write, like database.<br>(Whole object, partial writes discarded)</li>
						<li>Can be bytes ~ GB per object</li>
						<li>HTTP1.x has <b>per-request</b> latency overhead</li>
						<li>Remote cloud common (= more latency)</li>
					</ul>
				</section>
				<section>
					<ul>
						<li>Is the binary value a single scalar? One object? Collections of many objects? Up to you.</li>
						<li>Serialization and deserialization your responsibility</li>
						<li>Object type separation and schema changes your responsibility</li>
					</ul>
				</section>

				<section><!-- Key-value -->
					<h1>Key-value store</h1>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="1|4|6-7|9-10|1-13">
						import xyz_kv_lib
						db = xyz_kv_lib.connect(dbname='...', user=..., ...)
						
						x = # any sort of scalar value or object 

						x_key = str(x.id)
						db.set(x.id, serialize(x));

						y_key = 'MER-27620021'
						y = deserialize(db.get(y_key))
					</code></pre>
					<aside class="notes">
						1. The context object could be a single namespace or a table, it varies between different servers
						2. Use your own objects. But stereotypically you have to serialize and deserialize them; and if the objects are complex then there will be migration complexity - old records have the old format, new ones have the new, how to detect, update policy? Etc.
						3. set(), get(), delete(), list_keys() is all there is the idealized KV server
						4. There is no general query syntax to use to find values with property x or property y. There is only fetching by key, or iterating all keys.
					</aside>
				</section>
				<section>
					<ul>
						<li>KV store usually located near: &lt; 1 ms nearby network, or localhost with app.</li>
						<li>Values typically bytes, kilobytes size. Not MB or larger.</li>
						<li>1M+ ops/sec possible if localhost RAM, or high network quality</li>
						<li>Transaction support often available</li>
					</ul>
				</section>
				<section>
					<ul>
						<li>Is the binary value a single scalar? One object? Arrays/maps/etc. of objects? Up to you.</li>
						<li>If scalar (de)serilization is simple</li>
						<li>Otherwise (de)serilization and schema changes serious responsibility</li>
					</ul>
				</section>
				<!-- section>
					<div>FYI Many database storage engines wrap a key-value store</div>
					<div>Eg. MongoDB wraps WiredTiger</div>
					<div>Eg. RocksDB built around a LevelDB-forked core</div>
				</section -->

				<section><!-- Timeseries db -->
					<h1>Timeseries database</h1>
				</section>	
				<section>
					<pre><code data-trim data-line-numbers="2-3|5-13|1-15">
						# write to timeseries db
						import xyz_tsdb_lib
						tsdb_conn = tsdb.open(host_url)
			
						metric_sample = [
							{"name": "page_requests", "labels": [ "abc": ..., ...],
							 "timestamps": [t₁, t₂, t₃, ...], 
							 "values":     [x₁, x₂, x₃, ...]},
							{"name": "page_requests", "labels": [ "abc": ..., ...],
							 "timestamps": [t₁, t₂, t₃, ...], 
							 "values":     [y₁, y₂, y₃, ...]},
							...
						]​

						ts_write_ok = tsdb_conn.put(metric_sample)
					</code></pre>
					<aside class="notes">
						The write side is simple.
						Can also be single time sample instead of span.
						1. Data is typically inserted by other continuously-running services. Because monitoring seems to be the most common
						2. There are both HTTP and custom or ODBC db drive type connections. The HTTP ones don't seem require auth by default. At least the ones typically used for monitoring.
						3. A time range to query on and the resolution (interval) to return the timeseries samples is always required.
						QuestDB example of insert with the ILP (InfluxDb line protocol) API is pretty similar: https://questdb.io/docs/develop/insert-data/
					</aside>
				</section>
				<section>
					<div>The timeseries key datatype is a series of<br/>{timestamp, numeric value} pairs</div>
					<div>Series has a name and type (gauge or counter), and extra fields (labels) for filtering / grouping later</div>
				</section>
				<!-- section>
					<h4>Gauge-type timeseries</h4>
					<div>Examples</div>
					<ul>
						<li>Number of open connections</li>
						<li>Current mem usage</li>
						<li>Current temperature</li>
						<li>Market price of stock XYZ</li>
					</ul>
					<div>The absolute value is meaningful</div>
				</section>
				<section>
					<h4>Counter-type timeseries</h4>
					<div>Examples</div>
					<ul>
						<li>Bytes transmitted</li>
						<li>Page views</li>
						<li>Total dollar receipts for foo-type products</li>
						<li>Total number of foo-type products sold</li>
					</ul>
					<div>The rate of change is meaningful</div>
				</section>
				<section>
					<h4>Histogram timeseries</h4>
					<div>Example: Bytes transmitted where size of response</div>
					<ul>
						<li>&lt;= 1kb</li>
						<li>&gt; 1kb, &lt;= 4kb</li>
						<li>&gt; 4kb, &lt;= 16kb</li>
						<li>etc.</li>
					</ul>
					<div>Basically auto-bucketed counter timeseries</div>
				</section -->
				<section>
					<pre><code data-trim data-line-numbers="2-3|5|7|8|10-17|1-17">
						# read plain timeseries
						import xyz_tsdb_lib
						tsdb_conn = tsdb.open(host_url)

						ts_span = [ Date("2023-03-24T08:55:00Z"), Date("2023-03-24T09:00:00Z") ]
						
						qry = '{name="page_requests",host="foo"}'
						 # or 'page_requests{host="foo"}' in a common convention
						ts_qry_result = tsdb_conn.query(ts_span, qry)

						print(ts_qry_result)
						> [ { name: "page_requests", labels: [ path: "/abc", host: "foo", ... ],
						      vals: [1679648096, 525262], [1679648111, 525745], ... ] }, 
								{ name: "page_requests", labels: [ path: "/def", host: "bar", ... ],
									vals: [1679648100,  24234], [1679648115,  24270], ... ] },
								{ name: "page_requests", labels: [ path: "/def", host: "foo", ... ],
								  vals: [1679648100,  24234], [1679648115,  24270], ... ] },
						  ]
					</code></pre>
					<aside class="notes">
						1. Data is typically inserted by other continuously-running services. Because monitoring seems to be the most common
						2. There are both HTTP and custom or ODBC db drive type connections. The HTTP ones don't seem require auth by default. At least the ones typically used for monitoring.
						3. A time range to query on and the resolution (interval) to return the timeseries samples is always required.
						4. There is query syntax. Aggregations on counter or gauge timeseries values. Often much more complex than this example.
						5. The result is an array of &lt;timestamp&gt;, &lt;value&gt; pairs. The timestamp is forced to be round steps by resolution argument and the values likewise - either latest-before value, nearest value, extrapolated value between two nearest samples, etc.
					</aside>
				</section>
				<section>
					<ul>
						<li>Simple read returns the original timestamp + value pairs</li>
						<li>Too many points of data for human reading</li>
						<li>Pass through to analytical algorithms etc.</li>
					</ul>
				</section>
				<section>
					<pre><code data-trim data-line-numbers="2-3|5-6|7|8|10-15|1-15">
						# aggregation query of timeseries db
						import xyz_tsdb_lib
						tsdb_conn = tsdb.open(host_url)

						ts_span = [ Date("2023-03-24T08:55:00Z"), Date("2023-03-24T09:00:00Z") ]
						resolution = "10s"
						qry = 'sum(rate(page_requests{host="foo"})) by (path)'
						ts_qry_result = tsdb_conn.query(ts_span, qry, resolution)

						print(ts_qry_result)
						> [ { metric: "page_requests", labels: [ path: "/abc" ],
						      vals: [1679648100, 4562], [1679648110, 1040], ... ] }, 
								{ metric: "page_requests", labels: [ path: "/def" ],
								  vals: [1679648100,   24], [1679648110,   56], ... ] },
						  ]
					</code></pre>
					<aside class="notes">
						<p>
							1. Data is typically inserted by other continuously-running services. Because monitoring seems to be the most common<br/>
							2. There are both HTTP and custom or ODBC db drive type connections. The HTTP ones don't seem require auth by default. At least the ones typically used for monitoring.<br/>
							3. A time range to query on and the resolution (interval) to return the timeseries samples is always required.<br/>
							4. There is query syntax. Aggregations on counter or gauge timeseries values. Often much more complex than this example.<br/>
							5. The result is an array of &lt;timestamp&gt;, &lt;value&gt; pairs. The timestamp is forced to be round steps by resolution argument and the values likewise - either latest-before value, nearest value, extrapolated value between two nearest samples, etc.
						</p>
					</aside>
				</section>
				<section>
					<ul>
						<li>Automatic selection of samples at requested time span and resolution.</li>
						<li>Aggregate functions &ndash; rate(), sum(), avg(), etc.</li>
						<li>Algebra syntax (+, -, /, *) allows adding two metrics, dividing one by another, group, etc.</li>
					</ul>
				</section>
				<section>
					<ul>
						<li>Timeseries are usually real-world measurements</li>
						<li>Complex aggregation formulae, but real-world intuition helps</li>
						<li>Most common use: visualization
							<svg style="stroke: #3d85de; fill: rgba(61, 133, 222, 0.3)" class="sparkline sparkline--blue sparkline--filled" width="100" height="30" stroke-width="3"><path class="sparkline--fill" d="M4 18.03 L 4 18.03 L 8.842105263157894 15.94 L 13.68421052631579 6.45 L 18.526315789473685 5 L 23.36842105263158 23.72 L 28.210526315789473 9.13 L 33.05263157894737 17 L 37.89473684210526 18.92 L 42.73684210526316 21.09 L 47.578947368421055 23.57 L 52.421052631578945 20.63 L 57.26315789473684 17.59 L 62.10526315789474 14.87 L 66.94736842105263 8.1 L 71.78947368421052 5.24 L 76.63157894736842 23.55 L 81.47368421052632 8.76 L 86.3157894736842 19.92 L 91.15789473684211 17.7 L 96 14.87 V 30 L 4 30 Z" stroke="none"></path><path class="sparkline--line" d="M4 18.03 L 4 18.03 L 8.842105263157894 15.94 L 13.68421052631579 6.45 L 18.526315789473685 5 L 23.36842105263158 23.72 L 28.210526315789473 9.13 L 33.05263157894737 17 L 37.89473684210526 18.92 L 42.73684210526316 21.09 L 47.578947368421055 23.57 L 52.421052631578945 20.63 L 57.26315789473684 17.59 L 62.10526315789474 14.87 L 66.94736842105263 8.1 L 71.78947368421052 5.24 L 76.63157894736842 23.55 L 81.47368421052632 8.76 L 86.3157894736842 19.92 L 91.15789473684211 17.7 L 96 14.87" fill="none"></path></svg>
						  or machine learning</li>
					</ul>
				</section>

				<section><!-- Graph dbs -->
					<h1>Graph database</h1>
				</section>
				<section>
					<pre><code data-trim>
						-- Connection etc. The same as SQL or a document db API
						
						p1 = [10320, "smith", "john"]
						db.execute("INSERT People (?)", new_p) 
						t1 = ["ACME", "UK", "Risk"]
						db.execute("INSERT TeamMembership(?, ?)", p1, t1))

						person_cur = db.execute(
							'SELECT Person-[TeamMembership]->Team(?)', t1);
						print(person_cur.fetchone())
						-- (10414, "kelly", "jane")
					</code></pre>
					<aside class="notes">
						<pre>
							KuzuDB:
								MATCH (a:User)-[e:Follows]->(b:User)
								RETURN a.name, e, b.name;
							Tigergraph:
								SELECT p
								FROM users:u-(Posted>)-:p
								WHERE u.id == uid;
						</pre>
					</aside>
				</section>
				<section>
					<p>Query and update syntax has expressions for edge and nodes (vertices)</p>
					<p>Otherwise just like other database APIs</p>
					<p>Most Graph DBs have SQL-ish query language.<br>
						A few have a Document API.</p>
					<aside class="notes">
						<p>
							KuzuDB are some examples of SQL-ish. https://kuzudb.com/docs/cypher/query-clauses/match.html<br/>
							ArrangoDB is mostly document db, but has parsed-QL-string API like SQL.<br/>
							TigerGraph is very document-like via the HTTP API. But also has "GSQL"<br/>
							TerminusDB has the document db API. https://terminusdb.com/docs/guides/reference-guides/document-interface .
						</p>
					</aside>
				</section>

				<section><!-- Embedded Storage Engine -->
					<h1>Embedded Storage Engine</h1>
				</section>
				<section>
					<p>Do you have an app like this?</p>
					<ul>
						<li>Multi-threaded app, low-latency goals</li>
						<li>App doesn't share data with other app instances</li>
						<li>Local filesystem durability is enough</li>
						<li>Very low-frills query engine is OK</li>
					</ul>
				</section>
				<section>
					<p>If "Yes", you can use an embedded storage engine</p>
				</section>
				<section>
					<div>"Can't I just save app data to disk myself?"</div>
					<p class="fragment">Yes, but if you use the prebuilt storage engine ...</p>
				</section>
				<section>
					<ul>
						<li>Multi-thread contention for same records</li>
						<li>Fast, guaranteed ACID consistency</li>
						<li>Best disk &RightArrowLeftArrow; memory algorithms for RAM cache</li>
						<li>Convenient management of secondary indexes</li>
						<li>Backup-copy function</li>
						<li>etc.</li>
					</ul>
				</section>
				<section>
					<div>'Lite' database example</div>
					<pre><code data-trim>
						#include &lt;stdio.h&gt;
						#include &lt;sqlite3.h&gt;
						
						static int fooIterateCb(void *NotUsed, int argc, char **argv, char **azColName){
						  int i;
						  for(i=0; i&lt;argc; i++){
						    printf("%s = %s\n", azColName[i], argv[i] ? argv[i] : "NULL");
							}
						  printf("\n");
						  return 0;
						}
						
						int main(int argc, char **argv){
						  sqlite3 *db;
							char *zErrMsg = 0;
						  int rc;
						
						  if( argc!=3 ){
						    fprintf(stderr, "Usage: %s DATABASE SQL-STATEMENT\n", argv[0]);
						    return(1);
						  }
						  rc = sqlite3_open("testdb", &db);
						  if( rc ){
						    fprintf(stderr, "Can't open database: %s\n", sqlite3_errmsg(db));
						     sqlite3_close(db);
						    return(1);
						  }
						  rc = sqlite3_exec(db, "SELECT * FROM foo", fooIterateCb, 0, &zErrMsg);
						  if( rc!=SQLITE_OK ){
						    fprintf(stderr, "SQL error: %s\n", zErrMsg);
						    sqlite3_free(zErrMsg);
						  }
						  sqlite3_close(db);
						  return 0;
						}
					</code></pre>
					<aside class="notes">
						Sample above from dfrom https://zetcode.com/db/sqlitec/
					</aside>
				</section>
				<section>
					<di>Key-value storage engine example</di>
					<pre><code data-trim>
						#include &lt;sys/types.h&gt;
						#include &lt;stdio.h&gt;
						#include &lt;stdlib.h&gt;
						#include &lt;string.h&gt;
						
						#include &lt;db.h&gt;
						
						#define	DATABASE	"access.db"
						int main __P((int, char *[]));
						int usage __P((void));
						
						int
						main(argc, argv)
							int argc;
							char *argv[];
						{
							extern int optind;
							DB *dbp;
							DBC *dbcp;
							DBT key, data;
							size_t len;
							int ch, ret, rflag;
							char *database, *p, *t, buf[1024], rbuf[1024];
							const char *progname = "ex_access";		/* Program name. */
							...
						}

						...
						/* Optionally discard the database. */
						if (rflag)
							(void)remove(database);
						
						/* Create and initialize database object, open the database. */
						if ((ret = db_create(&dbp, NULL, 0)) != 0) {
							fprintf(stderr,
									"%s: db_create: %s\n", progname, db_strerror(ret));
							return (EXIT_FAILURE);
						}
						dbp->set_errfile(dbp, stderr);
						dbp->set_errpfx(dbp, progname);
						if ((ret = dbp->set_pagesize(dbp, 1024)) != 0) {
							dbp->err(dbp, ret, "set_pagesize");
							goto err1;
						}
						if ((ret = dbp->set_cachesize(dbp, 0, 32 * 1024, 0)) != 0) {
							dbp->err(dbp, ret, "set_cachesize");
							goto err1;
						}
						if ((ret = dbp->open(dbp,
								NULL, database, NULL, DB_BTREE, DB_CREATE, 0664)) != 0) {
							dbp->err(dbp, ret, "%s: open", database);
							goto err1;
						}

						...
						...
					</code></pre>
					<aside class="notes">
						<ul>
							<li>Sample above from https://www.sqlite.org/quickstart.html. Others also at https://github.com/berkeleydb/libdb/blob/master/examples/c/ex_access.c</li>
							<li>'Leanstore evolution' 2023 paper:<br>
								3 LeanStore System Overview<br>
								Functionality and System Overview. LeanStore is a storage engine supporting basic key
								value operations (insert/update/delete, point/range lookup) and transactional semantics
								(begin, commit, rollback). This functionality is exposed through a C++ interface, i.e., as an
								embeddable library rather than a server process. Systems with comparable functionality
								include RocksDB and WiredTiger, which are often used as building blocks for full-blown</li>
						</ul>	
					</aside>
				</section>
				<section>
					<ul>
						<li>Latency and throughput very high</li>
						<li>API will probably be C</li>
						<li>Database engine objects inside app process</li>
						<li>Database service threads inside app process</li>
						<li>Database-internal issues will be app issues</li>
						<li>App crash = unclean db shutdown</li>
					</ul>
				</section>

				<section><!-- Mashups title -->
					<h2>&hellip; plus the<br>'Mashups'</h2>
				</section>
				<section>
					<h2>Multi-model</h2>
					<p>Data model in datastore is mixture of main types</p>
				</section>
				<section>
					<p>"A little bit of this in that"</p>
					<p>Eg. JSON-ish object data type in relational table field<br/>
						Good for: Old app A is relational, but document style will suit new app B</p>
					<p>Eg. Graph extension in relational or document databases</p>
				</section>
				<section>
					<div>Eg. JSON_OBJECT in a MySQL table</div>
					<pre><code data-trim data-line-numbers="1-5,15|6-14|1-15">
						INSERT INTO e_store.products (
							name, brand_id, category_id, 
							attributes
						) VALUES (
							"Desire", 2, 14, 
							JSON_OBJECT(
								"sim", "Micro-SIM",
								"network", JSON_ARRAY(
									"GSM" , "CDMA" , "HSPA" , "EVDO"
									) ,
								"dim_desc", "5.11 x 2.59 x 0.46 inches" ,
								"weight_kg", 0.143,
								..."
							)
						);
					</code></pre>
					<aside class="notes">
						JSON_OBJECT (not JSON) type of MySQL shown. https://www.digitalocean.com/community/tutorials/working-with-json-in-mysql
					</aside>
				</section>
				<section>
					<p>API change barely noticeable with some multi-model additions. Eg:</p>
					<ul>
						<li>Geo-spatial datatype for database field</li>
						<li>Text search in database tables<br>
							(simple tokenization of string; tokens &rightarrow; record)</li>
					</ul>
				</section>
				<section>
					<h2>Competing APIs for same datastore</h2>
					<p class="fragment">Rare, but deserve a special mention</p>
				</section>
				<section>
					<p>Example for MySQL by ODBC</p>
					<pre><code data-trim>
						db_conn = pyodbc.connect("Server=....;Database=test;....")
						cursor = db_conn.cursor()
						cursor.execute(
							"INSERT INTO my_table (name, dim_x) VALUES (?, ?)", 
							"foo", 19)
					</code></pre>
					<p>&hellip; vs X DevAPI</p>
					<pre><code data-trim>
						mySession = mysqlx.get_session({"host": ..., ...  })
						myDb = mySession.get_schema('test')
						myColl = myDb.get_collection('my_table')
						myColl.add({ 'name': 'foo', 'dim_x': 19 }).execute()
					</code></pre>
				</section>

				<section><!-- "Pros and cons" -->
					<h1>Review of Main features</h1>
				</section>

				<section><!-- Schema freedom -->
					<h2>Schema freedom</h2>
				</section>
				<section>
					<p>Some datastores restrict the schema, some don't.</p>
					<p style="font-family: monospace; font-size: 80%;">Unknown field &RightArrow; Reject &RightArrow; client runtime error</p>
					<p>vs.</p>
					<p style="font-family: monospace; font-size: 80%;">Unknown field &RightArrow; Accept&semi;<br>Surprise new field for other clients</p>
					<aside class="notes">
						If applications insert a record that has an unknown field or is missing a field that is mandatory the entire request will be rejected and exception sent back to the client application.
						Relational databases are well-known for popularizing schema control.
					</aside>
				</section>
				<section>
					<h4>Positives of server-side locked schema</h4>
					<ul>
						<li>All applications obey a single schema orthodoxy</li>
						<li>Cross-table parent-child relationships can be enforced</li>
					</ul>
				</section>
				<section>
					<h4>Negatives of server-side locked schema</h4>
					<p>Schema changes time-consuming</p>
					<ul>
						<li>Some downtime usually required</li>
						<li>App devs and DBA teams need to work in sync.<br>
							(Typically adds many weeks to releases)</li>
					</ul>
				</section>
				<section>
					<h4>Schema-enforcing datastores</h4>
					<ul>
						<li>Relational databases</li>
						<li>Graph</li>
						<li class="na_ish">Timeseries</li>
					</ul>
				</section>
				<section>
					<h4>Schema-free datastores</h4>
					<p>(as commonly used, at least)</p>
					<ul>
						<li>Document databases</li>
						<li class="na_ish">Object stores</li>
						<li class="na_ish">Key-value</li>
						<li class="na_ish">Block / file</li>
					</ul>
				</section>

				<section><!-- Server-side analytical functions -->
					<h2>Aggregations / Analytical processing in the datastore</h2>
				</section>
				<section data-auto-animate>
					<h3>Minimal functionality of datastores</h3>
					<h4>Database</h4>
					<ul>
						<li>Create, drop, list tables</li>
						<li>Define fields in each table</li>
						<li>Update, select, delete and iterate records in table</li>
						<li>Select records by any field's values (not just primary key)</li>
						<li>Return subset of fields in records</li>
					</ul>
					<aside class="notes">
						<p>Different datastores offer different levels of server-side analytical processing.</p>
						<p>Because the functionality is implemented server-side this is not about Client APIs in sense, but there is correlation to the categories.</p>
					</aside>
				</section>
				<section data-auto-animate>
					<h3>Minimal functionality of datastores</h3>
					<h4 class="shrink_strike_fade">Database</h4>
					<h4>Object store, Key-value store, or Fileystem:</h4>
					<ul>
						<li class="shrink_strike_fade">Create, drop, list tables</li>
						<li>Put, retrieve, delete and list objects/files/values</li>
						<li class="shrink_strike_fade">Define fields in each table</li>
						<li class="shrink_strike_fade">Update, select, delete and iterate records in table</li>
						<li class="shrink_strike_fade">Select records by any field's values</li>
						<li>Single id/key for lookup</li>
						<li class="shrink_strike_fade">Return subset of fields in records</li>
					</ul>
					<aside class="notes">
						<p>Different datastores offer different levels of server-side analytical processing.</p>
						<p>Because the functionality is implemented server-side this is not about Client APIs in sense, but there is correlation to the categories.</p>
					</aside>
				</section>
				<section>
					<h3>Some datastores have no A.P.</h3>
					<p>By definition object stores, key-value stores, and fileystems provide no analytical processing</p>
					<p>Client app: Fetch whole dataset, deserialize, run analytical function</p>
				</section>
				<section>
					<h3>Minimal database: Basically no A.P.</h3>
					<div>A hypothetical minimal database adds:</div>
					<ul>
						<li>Filter records by any field's values<br>(not just primary key)</li>
						<li>Return subset of fields in records</li>
					</ul>
					<aside class="notes">
						There are some toy databases like this. But just filtering isn't analytical work.
					</aside>
				</section>
				<section>
					<h3>Typical database: Plain aggregation</h3>
					<ul>
						<li>Transformations such as:
							<ul>
								<li>"SELECT COUNT(*) ..." -> return one number</li>
								<li>"SELECT MAX(*) ..." -> return one scalar value</li>
							</ul>
						</li>
						<li>Sorting such as:
							<ul>
								<li>"SELECT ... ORDER BY ... LIMIT 10"<br>
									tbl_abc.read({..}, {$sort: {...}, $limit: 10})</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<h3>Typical database: Plain aggregation (cont.)</h3>
					<ul>
						<li>Grouping aggregations:
							<ul>
								<li>"SELECT x, SUM(y) ... GROUP BY x ... HAVING ..."<br>
									tbl_abc.aggregate([{$match: {..}}, {$groupBy: {xxx, {$sum: ...}}}, ...])</li>
							</ul>
						</li>
					</ul>
				</section>
				<section>
					<h3>Mature database: Richer aggregation</h3>
					<ul>
						<li>Windowing aggregations:
							<ul>
								<li>"SELECT ... WINDOW foo AS (PARTITION BY ... PRECEDING / EXCLUDING / etc. ...)"<br>
									tbl_abc.aggregate([{...}, {$groupBy: {, $bucket:{opX, opY, ...}}, ...}}, ...])</li>
							</ul>
						</li>
						<li>Temporary tables and CTEs</li>
					</ul>
				</section>
				<section>
					<h3>Mature database: User funcs, simple algorithmic</h3>
					<ul>
						<li>Stored procedures</li>
						<li>User-defined functions</li>
					</ul>
					</ul>
				</section>
				<section>
					<h3>User experience</h3>
					<ul>
						<li>🙂 'Yay, database can do this for me!'</li>
						<li>😓 Aggregation syntax complex, different domain language</li>
						<li>🙁 Large aggregations impact latency for every client</li>
						<li>😞 Algorithmic capability low</li>
					</ul>
				</section>
				<section>
					<h3>Performance summary</h3>
					<ul>
						<li>😐 Load consolidated in database server</li>
						<li>🙂 Database record filtering efficient</li>
						<li>🙂 Database aggregation efficient</li>
						<li>🫤 Algorithmic processing not so efficient</li>
					</ul>
				</section>
				<!-- section>
					<ul>
						<li>Key-value and Object stores have none.</li>
						<li>Relational databases offer the most as a general rule, but there are some toy-sized relational databases that don't have much.</li>
						<li>Document databases offer a medium amount to a lot, depending on which.</li>
						<li>Timeseries databases have a lot, for their scope.</li>
						<li>Graph databases have a medium amount.</li>
					</ul>
				</section -->

				<!-- section><! -- Complex query language -- >
					<h2>Complexity level of API</h2>
				</section>
				<section>
					<h3>Simple functionality in datastore</h3>
					<p>Example: Classic, spartan key-value store</p>
					<p>Only four commands &ndash; PUT, GET, DELETE and LIST</p>
					<p>Keys and values are binary, need deserialization in the app</p>
					<p>Application developer obliged to code everything beyond single-record processing</p>
				</section>
				<section>
					<h3>Complex functionality in datastore</h3>
					<p>Example: RDBMS with fully-evolved SQL syntax suport. Stored Procedures. Secondary indexes. Query plan optimizations</p>
				</section -->

				<section><!-- Connections -->
					<h2>Connections</h2>
				</section>

				<section>
					<h3>Connection behaviour</h3>
					<p>The behaviour of an APIs client-server connections is an important feature.</p>
				</section>
				<section>
					<p>Connection behaviour is a different dimension to the code:</p>
					<ul>
						<li>You don't program it</li>
						<li>The client API either supports some connection mode or it doesn't.</li>
						<li>Using different modes is more often a matter of config files rather than code lines.</li>
					</ul>
				</section>
				<section>
					<p>Reading the previous slide the app developer might feel <i>"I'm powerless here; I'll skip thinking about it for now."</i></p>
					<p>Don't do that!</p>
				</section>
				<section>
					<p>Be aware of what you get with different client APIs - what they support significantly changes the way the application is managed in live operation / meets business requirements.</p>
				</section>

				<section>
					<h3>Connection Protocols</h3>
					<p>The type of network protocol a database / datastore client API uses affects</p>
					<ul>
						<li>Performance</li>
						<li>Failure response</li>
					</ul>
				</section>
				<section>
					<ul>
						<li>Custom wire protocol over TCP</li>
						<li>HTTP1.x</li>
						<li>HTTP2 Streaming</li>
						<ul>
							<li>GRPC</li>
						</ul>
						<!-- TODO is this list complete? Which datastore was GRPC used with again? Other than Kafka -->
					</ul>
				</section>
				<section>
					<p>Connections that stay open provide the best latency</p>
					<p>A TCP connection is also simple way to support session state</p>
				</section>
				<section>
					<p>Stateless, always-reconnecting protocols have higher latency, and authentication has extra work</p>
					<p>Simple proxies like HTTP load balancer can be used if datastore has 'weaker' consistency</p>
					<!-- p>Requests that use a new connection each time can be run without holding a TCP socket open indefinitely while the app runs. Less state allows broader network scenarios, eg. SaaS backends behind load balancers.</p -->
				</section>
				<section>
					<p>Client APIs that have automatic failover provide a large benefit for business continuity goals.</p>
					<p>Options to auto-retry interrupted requests save the app developer from writing a lot of error handling.</p>
				</section>
				<!--section>
					<p>Databases that have replication (primary+secondaries, or 'multi-master', or master+slave, main+standby, etc) often support rolling restart procedures. <b>This is a very important cost-of-operations advantage.</b></p>
					<p>If the client API seamlessly switches to new primary as the role changes during maintenance restarts you gain a huge operational advantage: DBAs can do maintenance such as upscaling to new servers, upgrading versions etc. without shutting down the frontend application. I.e. without stopping the business or dragging the app dev team into the task.</p>
				</section -->
				<section>
					<h2>Connection Security</h2>
					<ul>
						<li>Network encryption</li>
						<li>Authentication</li>
						<ul>
							<li>Classic account name + password</li>
							<li>Certificate</li>
							<li>Federated Authentication</li>
						</ul>
					</ul>
				</section>


				<section><!-- Client-side encryption -->
					<h1>Client-side encryption</h1>
				</section>
				<section>
					<p>Recent addition to at least one popular datastore</p>
					<ul>
						<li>Use a KMS that the client connects to independently of the datastore</li>
						<li>Encrypt data before writing to datastore; decrypt after retrieving from datastore</li>
					</ul>
					<p>Not a feature of the datastore - all built into the client-side API</p>
				</section>
				<!-- section>
					<p>To protect against information theft an application can store already-encrypted data in the database / datastore. Hacking the database server, or its backups, becomes pointless.</p>
					<p>Something to consider for applications that have security requirements so high they can't even trust the DBA team / DBAAS provider.</p>
					<p>Storing encrypted data makes the database 'dumber' though. Eg. querying a record set by range X1 to X2 cannot use efficient index lookup if "X" is an encrypted value</p>
				</!section>
				<section>
					<p>A few database APIs have added the crypto functions to the database API itself. This is an alternative to using a general-purpose crypto library to manually do the encryption and decryption steps.</p>
					<p>The API cannot auto-magic the work away though. The tech debt rises just as much as if you do manual encryption. The main burden is the key management, i.e. the IT security team needs to manage the certs and distribute new and periodically renewed certs to the application servers, and the application team have to work with them when they do that.</p>
				</section -->

				<section><!-- Data model -->
					<h2>Data model</h2>
				</section>
				<section>
					<p>For most datastores the data model has strong influence on API</p>
					<ul>For example:
						<li>Graph DB: Edges, vertices part of fundamental syntax</li>
						<li>Object or KV store: Limited ops = small API</li>
					</ul>
					<!-- p>These shown in the previous 'Look and feel' section.</p -->
				</section>
				<section>
					<p>Data model type usually determines API type.</p>
					<p>Most notable exception: These 2 data models are common in SQL-using databases</p>
					<ul>
						<li>Standard: Row-based, fixed field structures</li>
						<li>Column-based: Columns kept in sequential datastructures</li>
					</ul>
					<!-- div>The same SQL can be used with both. OLTP will be faster with row-based; OLAP can be much faster with column-based </div -->
				</section>
				<section>
					<p>Write and read performance in the datastore<br><i>strongly affected</i> by data model</p>
				</section>

				<section data-background-gradient="radial-gradient(#283b95, #17b2c3)"><!-- Summary -->
					<h1>Client APIs Summary</h1>
				</section>

				<section data-auto-animate>
					<p>Datastore Client APIs ...</p>
				</section>
				<section data-auto-animate>
					<p>Datastore Client APIs ...</p>
					<p>have a lot of variety</p>
				</section>

				<section data-auto-animate>
					<p>Datastore API choice exists</p>
				</section>
				<section data-auto-animate>
					<p>Datastore API choice exists</p>
					<p>But it is not just 'flavor'</p>
				</section>
				<section data-auto-animate>
					<p>Big differences in:</p>
					<ul>
						<li>App development ease</li>
					</ul>
				</section>
				<section data-auto-animate>
					<p>Big differences in:</p>
					<ul>
						<li>App development ease</li>
						<li>Deployment ease</li>
					</ul>
				</section>
				<section data-auto-animate>
					<p>Big differences in:</p>
					<ul>
						<li>App development ease</li>
						<li>Deployment ease</li>
						<li>Uptime guarantees</li>
					</ul>
				</section>
				<section data-auto-animate>
					<p>Big differences in:</p>
					<ul>
						<li>App development ease</li>
						<li>Deployment ease</li>
						<li>Uptime guarantees</li>
						<li>Security coverage</li>
					</ul>
				</section>

				<section data-auto-animate>
					<p>Datastore API choice does NOT exist after</p>
					<p>the backend datastore has been chosen.</p>
				</section>

				<section data-auto-animate>
					<p>So before a datastore is decided<br>
						<b>don't waste opportunity</b><br>
						to judge datastores by client API style as well.</p>
				</section>

				<section>
					<div>&mdash;</div>
				</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealHighlight, RevealNotes ],
			});
		</script>
	</body>
</html>
